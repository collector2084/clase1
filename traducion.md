**Web Scraping con Beautiful Soup**  
√çconos usados en este cuaderno

üîî **Pregunta**: Una pregunta r√°pida para ayudarte a entender lo que est√° pasando.  
ü•ä **Desaf√≠o**: Ejercicio interactivo. ¬°Trabajaremos en ellos durante el taller!  
‚ö†Ô∏è **Advertencia**: Aviso sobre aspectos complicados o errores comunes.  
üí° **Consejo**: C√≥mo hacer algo de manera m√°s eficiente o efectiva.  
üé¨ **Demostraci√≥n**: Presentando algo m√°s avanzado, para que veas para qu√© se puede usar Python.

### **Objetivos de aprendizaje**

1. **Reflexi√≥n**: ¬øHacer scraping o no hacerlo?
2. **Extracci√≥n y an√°lisis de HTML**
3. **Scraping de la Asamblea General de Illinois**

### **Hacer Scraping o No Hacer Scraping**

Cuando queremos acceder a datos desde la web, primero debemos verificar si el sitio web de nuestro inter√©s ofrece una API web. Plataformas como Twitter, Reddit y el New York Times ofrecen APIs. Si quieres aprender a usarlas, consulta el taller de APIs web en Python de D-Lab.

Sin embargo, a veces no existe una API web disponible. En estos casos, podemos recurrir al **web scraping**, donde extraemos el HTML subyacente de una p√°gina web para obtener directamente la informaci√≥n que necesitamos. Existen varios paquetes en Python que nos ayudan a realizar estas tareas. Nos enfocaremos en dos paquetes: **Requests** y **Beautiful Soup**.

Nuestro caso de estudio ser√° el scraping de informaci√≥n sobre los **senadores estatales de Illinois**, as√≠ como la lista de proyectos de ley que ha patrocinado cada senador. Antes de comenzar, explora estos sitios web para familiarizarte con su estructura.



### **Instalaci√≥n**

Utilizaremos dos paquetes principales: **Requests** y **Beautiful Soup**. Si a√∫n no los tienes instalados, puedes hacerlo con el siguiente comando:
```
%pip install requests
%pip install beautifulsoup4
```
Tambi√©n instalaremos el paquete **lxml**, que ayuda a mejorar el an√°lisis de datos que realiza Beautiful Soup. Puedes instalarlo con el siguiente comando:
```
%pip install lxml
```
```
#Import required libraries
from bs4 import BeautifulSoup
from datetime import datetime
import requests
import time
```

### **Extracci√≥n y An√°lisis de HTML**

Para hacer **web scraping** y analizar HTML con √©xito, seguiremos estos **cuatro pasos**:

1.  **Hacer una solicitud GET** 
2.  **Analizar la p√°gina con Beautiful Soup**   
3.  **Buscar elementos HTML** 
4.  **Obtener los atributos y el texto de estos elementos** 
    


### **Paso 1: Hacer una Solicitud GET para Obtener el HTML de una P√°gina**

Podemos usar la biblioteca **Requests** para:

1.  **Hacer una solicitud GET** a la p√°gina web.    
2.  **Leer el c√≥digo HTML** de la p√°gina.
    
El proceso de hacer una solicitud y obtener un resultado es similar al flujo de trabajo de una API web. Sin embargo, en este caso, estamos haciendo la solicitud **directamente al sitio web**, y **tendremos que analizar el HTML nosotros mismos**. Esto es diferente de obtener datos organizados en un formato m√°s estructurado, como **JSON** o **XML**.
```
#Make a GET request
req = requests.get('http://www.ilga.gov/senate/default.asp')
#Read the content of the server‚Äôs response
src = req.text
#View some output
print(src[:1000])
```
### **Paso 2: Analizar la P√°gina con Beautiful Soup**

Ahora usaremos la funci√≥n **BeautifulSoup** para **convertir la respuesta en una estructura de √°rbol HTML**. Esto nos devuelve un objeto llamado **soup object**, que contiene todo el HTML del documento original.

Si te encuentras con un error relacionado con una biblioteca de an√°lisis (**parser**), aseg√∫rate de que tienes instalado el paquete **lxml**, ya que proporciona a Beautiful Soup las herramientas necesarias para procesar el HTML.
```
#Parse the response into an HTML tree
soup = BeautifulSoup(src, 'lxml')
#Take a look
print(soup.prettify()[:1000])
```
La salida se ve bastante similar a la anterior, pero ahora est√° organizada en un **objeto soup**, lo que nos permite recorrer la p√°gina de manera m√°s sencilla y estructurada

### **Paso 3: Buscar Elementos HTML**

Beautiful Soup cuenta con varias funciones para encontrar componentes √∫tiles en una p√°gina. Beautiful Soup permite encontrar elementos por: 

-   **Etiquetas HTML** 
-   **Atributos HTML** 
-   **Selectores CSS**
    
#### **Buscar por Etiquetas HTML**

La funci√≥n **`find_all`** busca en el √°rbol de **soup** y devuelve **todos los elementos** que coincidan con una etiqueta HTML espec√≠fica.

¬øQu√© hace el siguiente ejemplo?
```
#Find all elements with a certain tag
a_tags = soup.find_all("a")
print(a_tags[:10])
```
Porque `find_all()` es el m√©todo m√°s popular en la API de b√∫squeda de Beautiful Soup, puedes usar un atajo para ello. Si tratas el objeto BeautifulSoup como si fuera una funci√≥n, entonces es lo mismo que llamar a `find_all()` en ese objeto.

Estas dos l√≠neas de c√≥digo son equivalentes:
```
a_tags = soup.find_all("a")
a_tags_alt = soup("a")
print(a_tags[0])
print(a_tags_alt[0])
```
¬øCu√°ntos enlaces obtuvimos?
```
print(len(a_tags))
```
¬°Eso es mucho! Muchos elementos en una p√°gina tendr√°n la misma etiqueta HTML. Por ejemplo, si buscas todo lo que tenga la etiqueta a, es probable que obtengas m√°s resultados, muchos de los cuales quiz√°s no desees. Recuerda que la etiqueta a define un hiperv√≠nculo, por lo que generalmente encontrar√°s muchos en cualquier p√°gina dada.

¬øQu√© pasar√≠a si quisi√©ramos buscar etiquetas HTML con ciertos atributos, como clases CSS particulares?

Podemos hacer esto a√±adiendo un argumento adicional a `find_all`. En el ejemplo a continuaci√≥n, estamos encontrando todas las etiquetas a y luego filtrando aquellas con `class_="sidemenu"`.
```
#Get only the 'a' tags in 'sidemenu' class
side_menus = soup("a", class_="sidemenu")
side_menus[:5]
```
Una forma m√°s eficiente de buscar elementos en un sitio web es a trav√©s de un selector CSS. Para esto, tenemos que usar un m√©todo diferente llamado `select()`. Simplemente pasa una cadena al `.select()` para obtener todos los elementos que tengan esa cadena como un selector CSS v√°lido.

En el ejemplo anterior, podemos usar "a.sidemenu" como un selector CSS, que devuelve todas las etiquetas a con la clase sidemenu.
```
#Get elements with "a.sidemenu" CSS Selector.
selected = soup.select("a.sidemenu")
selected[:5]
```


### **Paso 4: Obtener Atributos y Texto de los Elementos**

Una vez que identificamos los elementos, queremos acceder a la informaci√≥n dentro de ellos. Generalmente, esto significa dos cosas:

-   **Texto** 
-   **Atributos** 

Obtener el texto dentro de un elemento es f√°cil. Solo necesitamos usar la propiedad **`.text`** de un objeto de etiqueta:
```
#Get all sidemenu links as a list
side_menu_links = soup.select("a.sidemenu")

#Examine the first link
first_link = side_menu_links[0]
print(first_link)

#What class is this variable?
print('Class: ', type(first_link))
```
¬°Es una etiqueta de Beautiful Soup! Esto significa que tiene un miembro de texto:
```
print(first_link.text)
```
A veces queremos el valor de ciertos atributos. Esto es particularmente relevante para las etiquetas a, o enlaces, donde el atributo href nos indica a d√≥nde lleva el enlace.

üí° Consejo: Puedes acceder a los atributos de una etiqueta trat√°ndola como un diccionario:
```
print(first_link['href'])
```

## **Scraping la Asamblea General de Illinois**

Cr√©alo o no, esas son realmente las herramientas fundamentales que necesitas para raspar un sitio web. Una vez que dediques m√°s tiempo a familiarizarte con HTML y CSS, simplemente se trata de entender la estructura de un sitio web en particular y aplicar inteligentemente las herramientas de Beautiful Soup y Python.

Apliquemos estas habilidades para raspar la 98¬™ Asamblea General de Illinois.

Espec√≠ficamente, nuestro objetivo es raspar informaci√≥n sobre cada senador, incluyendo su nombre, distrito y partido.

### **Scrapear y Analizar la P√°gina Web**

Ahora vamos a **scrapear** y **parsear** la p√°gina web utilizando las herramientas que aprendimos en la secci√≥n anterior. 
```
#Make a GET request
req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')
#Read the content of the server‚Äôs response
src = req.text
#Soup it
soup = BeautifulSoup(src, "lxml")
```
### **Buscar los elementos de la tabla**

Nuestro objetivo es obtener los elementos de la tabla en la p√°gina web. Recuerda: las filas se identifican por la etiqueta **tr**. Vamos a usar **find_all** para obtener estos elementos.
```
#Get all table row elements
rows = soup.find_all("tr")
len(rows)
```
‚ö†Ô∏è **Advertencia:** Ten en cuenta que **`find_all`** obtiene **todos** los elementos con la etiqueta `<tr>`. Sin embargo, solo queremos algunos de ellos.

Si usamos la funci√≥n **"Inspeccionar"** en Google Chrome y observamos con atenci√≥n, podemos utilizar **selectores CSS** para obtener solo las filas que nos interesan.

Espec√≠ficamente, queremos **las filas internas** de la tabla.
```
#Returns every ‚Äòtr tr tr‚Äô css selector in the page
rows = soup.select('tr tr tr')

for row in rows[:5]:
    print(row, '\n')
```
Parece que necesitamos todo despu√©s de las **primeras dos filas**.

Comencemos trabajando con una sola fila y, a partir de ah√≠, construiremos nuestro bucle. 
```
example_row = rows[2]
print(example_row.prettify())
```
Desglosaremos esta fila en sus **celdas/columnas** usando el m√©todo **`select`** con **selectores CSS**.

Si observamos detenidamente el HTML, hay varias formas de hacerlo:

1.  Podemos identificar las celdas por su etiqueta **`td`**.    
2.  Podemos usar el nombre de la clase **`.detail`**.    
3.  Podemos combinar ambos y usar el selector **`td.detail`**. 


```
for cell in example_row.select('td'):
    print(cell)
print()

for cell in example_row.select('.detail'):
    print(cell)
print()

for cell in example_row.select('td.detail'):
    print(cell)
print()
```
Podemos confirmar que todas estas opciones producen el mismo resultado. 
```
assert example_row.select('td') == example_row.select('.detail') == example_row.select('td.detail')
```
Utilicemos el selector **`td.detail`** para ser lo m√°s espec√≠ficos posible. 
```
#Select only those 'td' tags with class 'detail' 
detail_cells = example_row.select('td.detail')
detail_cells
```
La mayor parte del tiempo, estamos interesados en el texto real de un sitio web, no en sus etiquetas. Recuerda que para obtener el texto de un elemento HTML, usamos el atributo text:
```
#Keep only the text in each of those cells
row_data = [cell.text for cell in detail_cells]

print(row_data)
```
¬°Se ve bien! Ahora solo necesitamos usar nuestros conocimientos b√°sicos de Python para obtener los elementos de esta lista que queremos. Recuerda, queremos el nombre del senador, su distrito y su partido.
```
print(row_data[0]) # Name
print(row_data[3]) # District
print(row_data[4]) # Party
```
### **Eliminar Filas No Deseadas**

Vimos al principio que no todas las filas que obtuvimos corresponden realmente a un senador. Necesitaremos hacer algo de limpieza antes de poder continuar. Echa un vistazo a algunos ejemplos:
```
print('Row 0:\n', rows[0], '\n')
print('Row 1:\n', rows[1], '\n')
print('Last Row:\n', rows[-1])
```
Cuando escribamos nuestro bucle **_for_**, queremos que solo se aplique a las filas relevantes. Por lo tanto, necesitaremos filtrar las filas irrelevantes. La forma de hacerlo es compararlas con las filas que s√≠ queremos, ver en qu√© se diferencian y luego formular una condici√≥n basada en esas diferencias.

Como puedes imaginar, hay muchas formas posibles de hacerlo, y depender√° del sitio web. Mostraremos algunas aqu√≠ para darte una idea de c√≥mo hacerlo.
```
#Bad rows
print(len(rows[0]))
print(len(rows[1]))

#Good rows
print(len(rows[2]))
print(len(rows[3]))
```
Quiz√°s las filas correctas tengan una longitud de 5. Vamos a comprobarlo:
```
good_rows = [row for row in rows if len(row) == 5]
#Let's check some rows
print(good_rows[0], '\n')
print(good_rows[-2], '\n')
print(good_rows[-1])
```
Encontramos una fila de pie de p√°gina en nuestra lista que queremos evitar. Probemos otra cosa:
```
rows[2].select('td.detail') 
```
```
#Bad row
print(rows[-1].select('td.detail'), '\n')

#Good row
print(rows[5].select('td.detail'), '\n')

#How about this?
good_rows = [row for row in rows if row.select('td.detail')]

print("Checking rows...\n")
print(good_rows[0], '\n')
print(good_rows[-1])
```
¬°Parece que encontramos algo que funcion√≥!

### **Recorri√©ndolo Todo**

Ahora que hemos visto c√≥mo obtener los datos que queremos de una fila y c√≥mo filtrar las filas que no necesitamos, junt√©moslo todo en un bucle.
```
#Define storage list
members = []

#Get rid of junk rows
valid_rows = [row for row in rows if row.select('td.detail')]

#Loop through all rows
for row in valid_rows:
    # Select only those 'td' tags with class 'detail'
    detail_cells = row.select('td.detail')
    # Keep only the text in each of those cells
    row_data = [cell.text for cell in detail_cells]
    # Collect information
    name = row_data[0]
    district = int(row_data[3])
    party = row_data[4]
    # Store in a tuple
    senator = (name, district, party)
    # Append to list
    members.append(senator)
 ```
```
#Should be 61
len(members)
```
Echemos un vistazo a lo que tenemos en **_members_**.
```
print(members[:5])
```
